{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gdp/data/DeepPurpose0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, average_precision_score, f1_score\n",
    "from lifelines.utils import concordance_index\n",
    "from scipy.stats import pearsonr\n",
    "import pickle \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from torch.distributions import Normal\n",
    "\n",
    "torch.manual_seed(2)  \n",
    "np.random.seed(3)\n",
    "from prettytable import PrettyTable\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "print(os.path.abspath('.'))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from DeepPurpose import utils, models, dataset\n",
    "from DeepPurpose.models import *\n",
    "\n",
    "#from modules import *\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, init_dim):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=init_dim,\n",
    "            out_channels=32,\n",
    "            kernel_size=5,\n",
    "        padding=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=5,\n",
    "        padding=2)        \n",
    "        \n",
    "        self.conv3 = nn.Conv1d(\n",
    "            in_channels=64,\n",
    "            out_channels=128,\n",
    "            kernel_size=5,\n",
    "        padding=2)        \n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(init_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x_onehot):\n",
    "        x = self.conv1(F.leaky_relu(self.bn1(x_onehot)))\n",
    "        x = self.conv2(F.leaky_relu(self.bn2(x)))\n",
    "        x = self.conv3(F.leaky_relu(self.bn3(x)))\n",
    "        return F.leaky_relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN (nn.Module):\n",
    "\n",
    "    def __init__(self, init_dim):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.resnet = Bottleneck(init_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = F.adaptive_max_pool1d(x, output_size=1)  # b*128*1000>b*128\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderCNN(nn.Module):\n",
    "\tdef __init__(self, latent_feature_dim, init_dim, kernel_size=5):\n",
    "\t\tsuper(DecoderCNN, self).__init__()       \n",
    "\t\tclass View(nn.Module):\n",
    "\t\t\tdef __init__(self, shape):\n",
    "\t\t\t\tsuper(View, self).__init__()\n",
    "\t\t\t\tself.shape = shape\n",
    "\t\t\tdef forward(self, x):\n",
    "\t\t\t\treturn x.view(*self.shape)\n",
    "\n",
    "\t\tself.fc1 = nn.Sequential(\n",
    "\t\t\tnn.Linear(latent_feature_dim, 128),            \n",
    "\t\t\tnn.BatchNorm1d(128),\n",
    "\t\t\tnn.LeakyReLU())                      \n",
    "                            \n",
    "\t\tself.network = nn.Sequential(\n",
    "\t\t\tnn.Linear(128, 128 * 1000),            \n",
    "\t\t\tView(shape=(-1, 128, 1000)),            \n",
    "\t\t\tnn.BatchNorm1d(128),\n",
    "\t\t\tnn.LeakyReLU(),\n",
    "\t\t\tnn.ConvTranspose1d(in_channels = 128,out_channels = 64, kernel_size=5, padding = kernel_size//2),\n",
    "\n",
    "\t\t\tnn.BatchNorm1d(64),\n",
    "\t\t\tnn.LeakyReLU(),\n",
    "\t\t\tnn.ConvTranspose1d(in_channels = 64,out_channels = 32, kernel_size=5, padding = kernel_size//2),\n",
    "\n",
    "\t\t\tnn.BatchNorm1d(32),\n",
    "\t\t\tnn.LeakyReLU(),\n",
    "\t\t\tnn.ConvTranspose1d(in_channels = 32,out_channels = 26, kernel_size=5, padding = kernel_size//2))\n",
    "        \n",
    "\tdef forward(self, in_x):\n",
    "\t\tin_x=self.fc1(in_x)\n",
    "\t\tx=self.network(in_x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ONE_HOT_Encoder(train, batch_size, **config):\n",
    "   \n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\tBATCH_SIZE = batch_size   \n",
    "\tparams = {'batch_size': BATCH_SIZE, 'shuffle': False, 'drop_last': False}\n",
    "\ttraining_generator = data.DataLoader(data_process_loader(train.index.values, train.Label.values, train, **config), **params)\n",
    "\n",
    "\tv_f_D = torch.tensor([], dtype=torch.float)\n",
    "\tv_f_P = torch.tensor([], dtype=torch.float)    \n",
    "\ty_label = torch.tensor([], dtype=torch.float) \n",
    "\tfor i, (v_d, v_p, label) in enumerate(training_generator):\n",
    "\t\tif target_encoding == 'Transformer':\n",
    "\t\t\tv_p = v_p\n",
    "\t\telse:\n",
    "\t\t\tv_p = v_p.float() \n",
    "\t\tif drug_encoding == \"MPNN\" or drug_encoding == 'Transformer':\n",
    "\t\t\tv_d = v_d\n",
    "\t\telse:\n",
    "\t\t\tv_d = v_d.float()               \n",
    "\n",
    "\t\tv_D = v_d\n",
    "\t\t#print(v_D.shape)\n",
    "\t\tv_P = v_p\n",
    "\t\t#print(v_P.shape)\n",
    "\n",
    "\t\tv_f_D = torch.cat([v_f_D, v_D.detach()], dim=0)\n",
    "       \n",
    "\t\tv_f_P = torch.cat([v_f_P, v_P.detach()], dim=0)\n",
    "\t\ty_label = torch.cat([y_label, torch.from_numpy(np.array(label)).float()], dim=0)\n",
    "\treturn v_f_D, v_f_P, y_label.float().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, init_dim, latent_feature_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.h2mu = nn.Linear(128, latent_feature_dim)\n",
    "        self.h2log_var = nn.Linear(128, latent_feature_dim)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_normal(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            if isinstance(m, nn.Conv1d): \n",
    "                init.xavier_normal(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + (std * eps)\n",
    "        return z\n",
    "    \n",
    "    def loss_function(self, features_reconstruction, features, mu, log_var):\n",
    "        #re_loss = nn.MSELoss(reduction = 'sum')\n",
    "        re_loss = nn.BCEWithLogitsLoss(reduction = 'sum')  \n",
    "        reconstruction_loss = re_loss(features_reconstruction.reshape(-1, 26000), features.reshape(-1, 26000))/features_reconstruction.shape[0]\n",
    "        KLD = -0.5 * torch.sum(1. + log_var - torch.pow(mu, 2) - torch.exp(log_var))/mu.shape[0]\n",
    "        return reconstruction_loss, KLD\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x).squeeze(-1)\n",
    "        mu = self.h2mu(h)\n",
    "        log_var = self.h2log_var(h)       \n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        latent_representation = z\n",
    "        features_reconstruction = self.decoder(z)\n",
    "        #print(z)\n",
    "        #print(features_reconstruction)\n",
    "        re_loss, KLD=self.loss_function(features_reconstruction, x, mu, log_var)\n",
    "        return latent_representation, features_reconstruction, mu, log_var, re_loss, KLD\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \n",
    "        h = self.encoder(x).squeeze(-1)\n",
    "        mu = self.h2mu(h)\n",
    "        log_var = self.h2log_var(h)       \n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in total: 51863 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 6677\n",
      "drug encoding finished...\n",
      "encoding protein...\n",
      "unique target sequence: 766\n",
      "protein encoding finished...\n",
      "splitting dataset...\n",
      "Done.\n",
      "DTC subset Done!----------------------------------\n",
      "in total: 66434 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 10661\n",
      "drug encoding finished...\n",
      "encoding protein...\n",
      "unique target sequence: 1413\n",
      "protein encoding finished...\n",
      "splitting dataset...\n",
      "Done.\n",
      "DB Done!-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#DTC\n",
    "drug_encoding, target_encoding = 'CNN', 'CNN'\n",
    "\n",
    "DTC_sub=pd.read_csv('VAE_data/df_DTC_undrugs_targets.csv')\n",
    "Smile = DTC_sub['SMILES'].values\n",
    "Target = DTC_sub['Target_Sequence'].values\n",
    "y = DTC_sub['Label'].values\n",
    "train, val, test = utils.data_process(Smile, Target, y,\n",
    "                                drug_encoding, target_encoding, \n",
    "                                split_method='train_full',frac=[0.7,0.1,0.2],\n",
    "                                random_seed = 1)\n",
    "\n",
    "print('DTC subset Done!----------------------------------')\n",
    "#DB\n",
    "Smile2 = np.load(\"VAE_data/DB_smiles.npy\", allow_pickle=True)\n",
    "Target2 = np.load(\"VAE_data/DB_targets.npy\", allow_pickle=True)\n",
    "y2 = np.load(\"VAE_data/DB_y.npy\", allow_pickle=True)\n",
    "train2, val2, test2 = utils.data_process(Smile2, Target2, y2,\n",
    "                                drug_encoding, target_encoding, \n",
    "                                split_method='train_full',frac=[0.7,0.1,0.2],\n",
    "                                random_seed = 1)\n",
    "print('DB Done!-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode data for DTC\n",
    "config = utils.generate_config2(drug_encoding = drug_encoding, target_encoding = target_encoding)\n",
    "v_f_D, v_f_P, y = ONE_HOT_Encoder(train, 1024, **config)\n",
    "\n",
    "#encode data for DB\n",
    "config = utils.generate_config2(drug_encoding = drug_encoding, target_encoding = target_encoding)\n",
    "v_f_D2, v_f_P2, y2 = ONE_HOT_Encoder(train2, 1024, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_f_D_all = torch.cat([v_f_D, v_f_D2], dim=0)\n",
    "v_f_P_all = torch.cat([v_f_P, v_f_P2], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,d,c = v_f_P_all.shape\n",
    "s=int(0.1*n)\n",
    "index = torch.LongTensor(random.sample(range(n), s))\n",
    "#v_f_D_P = torch.cat([v_f_D, v_f_P], dim=1)\n",
    "v_f_D_val = torch.index_select(v_f_D_all, 0, index)\n",
    "#y_val = torch.index_select(y, 0, index)\n",
    "v_f_P_val = torch.index_select(v_f_P_all, 0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置模型参数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "latent_feature_dim = 10\n",
    "init_dim=26\n",
    "\n",
    "model = VAE(EncoderCNN(init_dim), DecoderCNN(latent_feature_dim, init_dim, kernel_size=5),init_dim, latent_feature_dim).to(device)\n",
    "#print(model)\n",
    "\n",
    "#设置训练参数\n",
    "LR = 1e-4\n",
    "epochs = 200\n",
    "batch_size=256\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)#, weight_decay= 1e-5 \n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.5, patience=4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = TensorDataset(v_f_P_all)\n",
    "train_loader = DataLoader(train_ids, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_ids = TensorDataset(v_f_P_val)\n",
    "val_loader = DataLoader(val_ids, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids2 = TensorDataset(v_f_D_all)\n",
    "train_loader2 = DataLoader(train_ids2, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_ids2 = TensorDataset(v_f_D_val)\n",
    "val_loader2 = DataLoader(val_ids2, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "loss_epoch = []\n",
    "loss_val_epoch=[]\n",
    "best_model = copy.deepcopy(model)\n",
    "best_loss = 1e8\n",
    "\n",
    "t_start = time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        x = data[0]\n",
    "        x = x.to(device)\n",
    "        embedding, features_reconstruction, mu, log_var, re_loss, KLD_loss = model(x)\n",
    "        loss = re_loss + 1*KLD_loss\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        t_now = time()\n",
    "        # print statistics every 100 batch\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], loss = {:.4f}, loss_re = {:.4f}, loss_KLD = {:.4f}, Total time = {:.4f} hours'.format(epoch, epochs, loss.item(), re_loss.item(), KLD_loss.item(), int(t_now - t_start)/3600))\n",
    "\n",
    "        \n",
    "    loss_epoch.append(np.mean(train_loss))\n",
    "       \n",
    "    # validate, select the best model up to now\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for val_batch_idx, val_data in enumerate(val_loader):\n",
    "            val_x = val_data[0]\n",
    "            val_x = val_x.to(device)\n",
    "            val_embedding, val_features_reconstruction, val_mu, val_log_var, val_re_loss, val_KLD_loss = model(val_x)\n",
    "            val_loss = val_re_loss + val_KLD_loss\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        # 对和求平均，得到平均损失\n",
    "        loss_val_epoch.append(np.mean(val_losses))\n",
    "        #if val_avg_loss < best_loss:\n",
    "        #    best_model = copy.deepcopy(model)\n",
    "        #    best_loss = val_avg_loss\n",
    "    print('Valerate at Epoch ' + str(epoch) + ' with loss ' + str(np.mean(val_losses))[:7])  \n",
    "#----------------------------------------\n",
    "fontsize = 16\n",
    "iter_num = list(range(1,len(loss_epoch)+1))\n",
    "plt.figure(0)\n",
    "plt.plot(iter_num, loss_epoch, \"bo-\")\n",
    "plt.xlabel(\"epoch\", fontsize = fontsize)\n",
    "plt.ylabel(\"train_loss\", fontsize = fontsize)\n",
    "\n",
    "fontsize = 16\n",
    "iter_num = list(range(1,len(loss_val_epoch)+1))\n",
    "plt.figure(1)\n",
    "plt.plot(iter_num, loss_val_epoch, \"mo-\")\n",
    "plt.xlabel(\"epoch\", fontsize = fontsize)\n",
    "plt.ylabel(\"val_loss\", fontsize = fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'vae_protein_dtc_db.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('./vae_protein_dtc_db.pt')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################unique target#####################################\n",
    "#Dtc\n",
    "AA = pd.Series(train['Target Sequence'].unique()).apply(trans_protein)\n",
    "AA2 = AA.apply(protein_2_embed)\n",
    "\n",
    "target_onehot_np = np.stack(AA2.values, axis=0)\n",
    "target_onehot_th = torch.from_numpy(target_onehot_np)\n",
    "\n",
    "#Db\n",
    "AA_dtc = pd.Series(train2['Target Sequence'].unique()).apply(trans_protein)\n",
    "AA2_dtc = AA_dtc.apply(protein_2_embed)\n",
    "\n",
    "target_onehot_db = np.stack(AA2_dtc.values, axis=0)\n",
    "target_onehot_db = torch.from_numpy(target_onehot_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_ids1 = TensorDataset(target_onehot_th)\n",
    "retrain_loader1 = DataLoader(retrain_ids1, batch_size=batch_size, shuffle=False) #DTC\n",
    "\n",
    "retrain_ids2 = TensorDataset(target_onehot_db)\n",
    "retrain_loader2 = DataLoader(retrain_ids2, batch_size=batch_size, shuffle=False) #DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-encoder\n",
    "features_vae = torch.tensor([], dtype=torch.float)\n",
    "for batch_idx, data in enumerate(retrain_loader2):\n",
    "    x = data[0]\n",
    "    x = x.float().to(device)\n",
    "    feature_vae = model.predict(x)\n",
    "    features_vae = torch.cat([features_vae, feature_vae.cpu().detach()], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(features_vae, 'features_target_db_dtcdb_vae.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
